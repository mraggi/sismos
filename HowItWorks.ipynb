{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inversion\n",
    "\n",
    "Suppose you had a set of points $P=\\{p_1, p_2, \\dots, p_n\\}$ on the plane, but for which you didn't know the coordinates. You only know the relative pairwise euclidean distances, and only approximately. That is, for points $p_i, p_j \\in P$, we know an approximation $d_{ij} \\approx d(p_i,p_j)$, where $d$ denotes the euclidean distance. How do we construct $P$? This is useful in earthquake analysis, apparently.\n",
    "\n",
    "## Problem statement\n",
    "Given a symmetric matrix $D \\in \\mathbb{R}^{n\\times n}$, produce a set of $n$ points in 2d-space for which the pairwise euclidean distance matrix matches $D$ as \"closely\" as possible, where the definition of \"closely\" might mean euclidean distance (in $\\mathbb{R}^{n\\times n}$) or even L1-distance.\n",
    "\n",
    "## Posing the problem as an optimization problem\n",
    "\n",
    "This is clearly an optimization problem: For every set of points $P \\in \\mathbb{R}^{n \\times 2}$, find the pairwise distance matrix $D_P$, where $D_P[i,j] = d(p_i,p_j)$. Among all such $P$, find the ones that minimize the objective function $d(D_P, D)$. If $L_1$ distance is desired, then minimize $\\sum |D_P-D|$.\n",
    "\n",
    "In practice, there are many methods that can and do find good solutions. Even simple gradient descent often manages to find a good solution. However, experimentally, we found the best results using differential evolution [1].\n",
    "\n",
    "### A quick overview of differential evolution\n",
    "\n",
    "This optimization method, in a nutshell, maintains a **population** of **individuals** (*i.e.* candidate solutions) that gradually *improve*. In our case, *individuals* are points in $\\mathbb{R}^{n \\times 2}$. Each generation is constructed from the previous generation. In this sense, differential evolution can be thought of as a genetic algorithm.\n",
    "\n",
    "To perform differential evolution, start with a population of random individuals. Then succesively repeat the following procedure: In each generation, for each individual $P$ in the current population, we choose 3 other individuals $A,B,C$ (from the current population) and use them to construct a new candidate individual, which we'll call $\\tilde{P}$. This candidate $\\tilde{P}$ is compared against $P$, and if appropiate, fully replaces $P$ in the next generation. This process continues until no more improvements are to be found.\n",
    "\n",
    "#### How to combine individuals.\n",
    "\n",
    "Specifically, given individuals $P, A, B, C$, produce \n",
    "    $$P' = A + \\lambda(B-C)$$ \n",
    "where $\\lambda$ is a real number (often chosen experimentally). Then, for each coordinate, choose either $p_i$ or $p_i'$ randomly to form a new element $\\tilde{P}$. If the objective function turns out to be better for $\\tilde{P}$ than it was for $P$, replace $P$ by $\\tilde{P}$ in the next generation.\n",
    "\n",
    "For further details, see the cited text.\n",
    "\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "[1] Storn, R.; Price, K. (1997). \"Differential evolution - a simple and efficient heuristic for global optimization over continuous spaces\". Journal of Global Optimization. 11 (4): 341â€“359. doi:10.1023/A:1008202821328."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
